<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Cupdish.com</title>
  
  <subtitle>杯子与盘子</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://cupdish.com/"/>
  <updated>2018-12-20T07:44:41.489Z</updated>
  <id>http://cupdish.com/</id>
  
  <author>
    <name>Victor Zhang</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Denoising Distantly Supervised Open-Domain Question Answering 阅读报告</title>
    <link href="http://cupdish.com/2018/12/20/denoising-review/"/>
    <id>http://cupdish.com/2018/12/20/denoising-review/</id>
    <published>2018-12-20T03:19:42.000Z</published>
    <updated>2018-12-20T07:44:41.489Z</updated>
    
    <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Denoising Distantly Supervised Open-Domain Question Answering 这篇文章是 清华的林衍凯同学和刘知远老师团队在 ACL2018 上发表的文章， 主要解决 开放问答领域中的 远程监督的噪声问题。原文<a href="http://www.thunlp.org/~lyk/publications/acl2018_denoising.pdf" target="_blank" rel="noopener">链接</a></p><a id="more"></a><h2 id="模型动机"><a href="#模型动机" class="headerlink" title="模型动机"></a>模型动机</h2><p>1） 现在的阅读理解，严重依赖于提前提供好的段落，这个和现实中的问答情况不符。</p><blockquote><p>Despite their success, existing reading comprehension systems rely on pre-identiﬁed relevant texts, which do not always exist in real-world question answering (QA) scenarios.</p></blockquote><p>2) 针对上述情况，斯坦福的陈丹琦提出基于远程监督的开放御自动问答系统(DS-QA)。首先从维基百科当中找到相关的信息，然后使用阅读理解的技术去提取答案</p><blockquote><p>Chen et al. (2017) propose a distantly supervised open-domain question answering (DS-QA) system which uses information retrieval technique to obtain relevant text from Wikipedia, and then applies reading comprehension technique to extract the answer.</p></blockquote><p>3）DS-QA的严重的不足是：远程监督带来了大量的噪音。</p><blockquote><p>Although DS-QA proposes an effective strategy to collect relevant texts automatically, it always suffers from the noise issue.</p></blockquote><p>4）一些现存的DS-QA系统，仅仅只是选取最相似的段落，然后对其进行阅读理解。而没有综合考虑多个段落。而往往一个问题的答案会在多个段落中提到（mentioned）</p><blockquote><p>These methods only extract the answer according to the most related paragraph. … In fact, the correct answer is often mentioned in multiple paragraphs, and different aspects of the question may be answered in several paragraphs.</p></blockquote><h2 id="本文贡献"><a href="#本文贡献" class="headerlink" title="本文贡献"></a>本文贡献</h2><p>本文提出了由粗到细的远程监督开放域问答模型（coarseto-ﬁne denoising model for DS-QA）,</p><ul><li>首先通过信息检索找到一些跟问题相关的段落</li><li>使用所有检索到的段落，做快速阅读，并淘汰噪音段落</li><li>使用精读算法（即阅读理解的算法）在上述段落中的各个段落进行精度。</li><li>综合各个段落得到的答案，给出最终答案</li></ul><p><img src="overview.png" alt="The overview of the model"></p><p>本文在Quasar-T、SearchQA、TriviaQA三个数据集效果上显示了显著的提升。并将代码公布到了<a href="https: //github.com/thunlp/OpenQA" target="_blank" rel="noopener">Github</a></p><h2 id="模型详解"><a href="#模型详解" class="headerlink" title="模型详解"></a>模型详解</h2><p>模型分为两部分：1）段落选择器（Paragraph Selector） 和2）段落阅读器（Paragraph Reader）。</p><p>假设问题为$q$，选择到的段落为$P={p_1,p_2,\cdots,p_m}$, 答案为$a$</p><p>段落选择器的目标是，给定$q$，计算$P$中每个段落是相关段落的概率，即：$\text{Pr}(p_i|q,P)$</p><p>段落阅读器的目标是，给定$q$，计算$P$中每个段落$p_i$中得到答案$a$的概率，即：$\text{Pr}(a|q,p_i)$</p><p>综上，给定$q$，得到答案$a$的概率为：<br>$$\text{Pr}(a|q,P)=\sum_{p_i\in P}{\text{Pr}(a|q,p_i)\text{Pr}(p_i|q,P)}$$</p><h3 id="段落选择器"><a href="#段落选择器" class="headerlink" title="段落选择器"></a>段落选择器</h3><h4 id="编码"><a href="#编码" class="headerlink" title="编码"></a>编码</h4><p>使用全连接网络（MLP）或循环神经网络（RNN）对问题和段落进行了编码。并对问题$q$做了<code>self attention</code></p><p>最后，通过交互、池化和<code>softmax</code>求的概率<br>$$\text{Pr}(p_i|q,P)=\text{softmax}(\text{max}_j{(p_i W q)})$$</p><h3 id="段落阅读器"><a href="#段落阅读器" class="headerlink" title="段落阅读器"></a>段落阅读器</h3><p>段落阅读实际上是选择一个<code>span</code>，选择<code>span</code>的头尾指针。即<br>$$\text{Pr}(a|q,p_i) = P_s(a_s)P_e(a_e)$$<br>其中，头尾概率的算法分别是：<br>$$P_s(j)\text{softmax}(p^j_i W_s q)$$<br>$$P_e(j)\text{softmax}(p^j_i W_e q)$$</p><p>因为多多个段落进行阅读，所以可能会有多个不同的答案。这里作者使用了两种方式，<br>一种是选取最大值，另一种是求和。<br>最大值<br>$$\text{Pr}(a|q,p_i) = \max_j P_s(a^j_s)P_e(a^j_e)$$<br>求和<br>$$\text{Pr}(a|q,p_i) = \sum_j P_s(a^j_s)P_e(a^j_e)$$</p><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>随时函数的定义即为$\text{Pr}(a|q,P)$的对数损失<code>logloss</code><br>$$L(\theta) = - \sum \text{Pr}(a|q,P)-\alpha R(P)$$</p><p>其中$R(P)$是<code>段落选择器</code>的正则化项。定义为概率$\text{Pr}(p_i|q,P)$和段落中实际包含答案个数的倒数$\chi_i = \frac{1}{c_P}$之间的正则化KL散度，即：</p><p>$$R(P)=\sum_{p_i \in P}\chi_i \log\frac{\chi_i}{\text{Pr}(p_i|q,P)}$$</p><p>文章使用了<code>Adam</code>优化器进行优化。</p><h2 id="一些细节"><a href="#一些细节" class="headerlink" title="一些细节"></a>一些细节</h2>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h2&gt;&lt;p&gt;Denoising Distantly Supervised Open-Domain Question Answering 这篇文章是 清华的林衍凯同学和刘知远老师团队在 ACL2018 上发表的文章， 主要解决 开放问答领域中的 远程监督的噪声问题。原文&lt;a href=&quot;http://www.thunlp.org/~lyk/publications/acl2018_denoising.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;链接&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="技术" scheme="http://cupdish.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="论文阅读报告" scheme="http://cupdish.com/categories/%E6%8A%80%E6%9C%AF/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E6%8A%A5%E5%91%8A/"/>
    
    
      <category term="深度学习" scheme="http://cupdish.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="论文阅读" scheme="http://cupdish.com/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
      <category term="ACL" scheme="http://cupdish.com/tags/ACL/"/>
    
      <category term="阅读理解" scheme="http://cupdish.com/tags/%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3/"/>
    
      <category term="自动问答" scheme="http://cupdish.com/tags/%E8%87%AA%E5%8A%A8%E9%97%AE%E7%AD%94/"/>
    
  </entry>
  
  <entry>
    <title>Attention is all you need 论文阅读报告及代码详解</title>
    <link href="http://cupdish.com/2018/03/28/attention-is-all-you-need/"/>
    <id>http://cupdish.com/2018/03/28/attention-is-all-you-need/</id>
    <published>2018-03-28T02:04:16.000Z</published>
    <updated>2018-11-03T16:17:22.747Z</updated>
    
    <content type="html"><![CDATA[<h2 id="文章简介"><a href="#文章简介" class="headerlink" title="文章简介"></a>文章简介</h2><p>这篇文章是Google公司谷歌大脑部门的一篇关于使用注意力机制<code>（Attention Mechanism）</code>来进行翻译的模型的研究，这篇文章最终发表在NIPS 2017上。整个模型还是基于 encode-decode的构架，但是attention不再和CNN以及RNN一起使用，而是单独使用。这个框架的名字叫做<code>Transformer</code></p><p><a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">论文链接</a></p><a id="more"></a><h2 id="Self-attention-Model"><a href="#Self-attention-Model" class="headerlink" title="Self-attention Model"></a>Self-attention Model</h2><p>自注意力模型是进两年来的热门模型，这个模型的想法挺有意思的。</p><p>attention 可以进行如下描述，一般为Query(Q)和Key(K)与Value(V)的对应的映射，其中key-value一一对应，在NLP的任务当中，通常 $K=V$, 整个attention 的结构可以看做下图。<br><img src="pic1.png" alt="图片1"></p><ol><li>模型通过比较Q和K的相似度，得到权重，通常的相似度计算方法如下<br> $$f(Q,K_i)=Q^TK_i$$ 点乘<br> $$f(Q,K_i)=Q^TWK_i$$ 权重<br> $$f(Q,K_i)=W[Q,K_i]$$ 拼接权重<br> $$f(Q,K_i)=v^T\text{tanh}(W Q+U K_i)$$ 感知器<br> 权重的计算方法如下：<br> $$\alpha_i = softmax(f(Q,K))=\frac{\text{exp}(f(Q,K_i))}{\sum_j\text{exp}(f(Q,K_i))}$$</li><li>根据上一步计算的权重，对value值进行加权计算，通常为加权求和，得到attention向量<br>$$AttentionValue(Q,K,V) = \sum_i{\alpha_i V_i}$$</li><li><code>self-attention</code> 模型就是自己对自己求attention，即$Q=K=V$, 但是论文中对其attention权重做了缩放，除去了$K$的纬度$\sqrt{d_k}$,得到<br>$$Attention(Q,K,V)=\text{softmax}(\frac{QK^T}{\sqrt{d_k}})V.$$</li></ol><p><img src="dot.png" alt="图片2：论文时用的attention方法"></p><h2 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h2><p>编码器由6个完全相同的层组成，每一个层有两个子层，一个是multi-headed attention层，另一个是一个position-wise的全连接层。在每一层中应用残差网络和正则化层的方法。残差网络就是将输入也链接到输出，使得训练更加有效。<br><img src="encoder.png" alt="图片3：编码器"></p><h3 id="Multi-headed-Attention"><a href="#Multi-headed-Attention" class="headerlink" title="Multi-headed Attention"></a>Multi-headed Attention</h3><p>论文提出了<code>Multi-headed Attention</code>,旨在对Q,K,V，在不同的空间的特征进行提取，其思想也比较简单，就是多做几次attention，其中每次attention的<strong>特征不共享</strong></p><p>$$\text{MultiHead}(Q,K,V) = \text{Concat}(\text{head}_i,\cdots,\text{head}_h)$$</p><p>其中每一个$\text{head}_i$计算方法如下<br>$$\text{head}_i = \text{Attention}(Q W^Q_i,K W^K_i,V W^V_i)$$</p><p>论文中使用了8个<code>heads</code></p><p><img src="multihead.png" alt="图片4：编码器"></p><h3 id="Position-Encoding"><a href="#Position-Encoding" class="headerlink" title="Position Encoding"></a>Position Encoding</h3><p>论文输入词向量时，不仅输入了词向量的序列，也考虑了词语之间的顺序和位置。</p><p>$$PE(pos,2i)=sin(\frac{pos}{10000^{2i/d_m}}) \\<br>PE(pos,2i+1)=cos(\frac{pos}{10000^{2i/d_m}})$$<br>其中，$d_m$表示输出的维数。<br>这样表示的原因是，对于$PE_{pos+k}$的词向量，可以是$PE_{pos}$向量的线性组合表示，这里利用了公式:<br>$$\sin(\alpha+\beta)=\sin\alpha \cos\beta + \cos\alpha \sin\beta \\<br>\cos(\alpha+\beta)=\cos\alpha \cos\beta - \sin\alpha \sin\beta$$</p><p>最后得到的位置向量，可以和原词向量拼接，也可以和词向量相加。</p><h3 id="Layer-Normalization"><a href="#Layer-Normalization" class="headerlink" title="Layer Normalization"></a>Layer Normalization</h3><p><code>Layer Normalization</code> 和<code>Batch Normalization</code> 想法相似，均为降低模型的复杂程度，缓解过拟合，加快训练效率。</p><h2 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h2><p>解码器也是由6个完全一样的层组成，每一个网络层由编码器中的两个子层组成外，还多了一个子层，是将编码器中的多头注意力加入到解码过程中。同时，使用了<code>Masking层</code>，对解码到当前位置<code>i</code>时，对解码器输入<code>i-1</code>之前的信息保留，对<code>i</code>之后的信息进行屏蔽<br><img src="decoder.png" alt="图片5:解码器"></p><h2 id="Tricks"><a href="#Tricks" class="headerlink" title="Tricks"></a>Tricks</h2><p>论文和代码实现，使用了一下小技巧：</p><ol><li>使用了大量的dropout以增强模型的泛化能力。<code>drop rate</code>选择为0.1</li><li>论文实验使用了8个GPU。</li></ol><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><h2 id="Code-分析"><a href="#Code-分析" class="headerlink" title="Code 分析"></a>Code 分析</h2><p>代码为 Kyubyong的版本，源代码地址：<a href="https://github.com/Kyubyong/transformer" target="_blank" rel="noopener">点我</a></p><h3 id="encoder-部分"><a href="#encoder-部分" class="headerlink" title="encoder 部分"></a>encoder 部分</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"encoder"</span>):</span><br><span class="line">                <span class="comment">## Embedding 词向量的计算</span></span><br><span class="line">                self.enc = embedding(self.x,</span><br><span class="line">                                      vocab_size=len(de2idx),</span><br><span class="line">                                      num_units=hp.hidden_units,</span><br><span class="line">                                      scale=<span class="keyword">True</span>,</span><br><span class="line">                                      scope=<span class="string">"enc_embed"</span>)</span><br><span class="line"></span><br><span class="line">                <span class="comment">## Positional Encoding 加入位置向量</span></span><br><span class="line">                self.enc += positional_encoding(self.x,</span><br><span class="line">                                      num_units=hp.hidden_units,</span><br><span class="line">                                      zero_pad=<span class="keyword">False</span>,</span><br><span class="line">                                      scale=<span class="keyword">False</span>,</span><br><span class="line">                                      scope=<span class="string">"enc_pe"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">                <span class="comment">## Dropout 对输入向量进行dropout处理，减少过拟合</span></span><br><span class="line">                self.enc = tf.layers.dropout(self.enc,</span><br><span class="line">                                            rate=hp.dropout_rate,</span><br><span class="line">                                            training=tf.convert_to_tensor(is_training))</span><br><span class="line"></span><br><span class="line">                <span class="comment">## Blocks，若干个相同的block</span></span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> range(hp.num_blocks):</span><br><span class="line">                    <span class="keyword">with</span> tf.variable_scope(<span class="string">"num_blocks_&#123;&#125;"</span>.format(i)):</span><br><span class="line">                        <span class="comment">### Multihead Attention，多头注意力机制</span></span><br><span class="line">                        self.enc = multihead_attention(queries=self.enc,</span><br><span class="line">                                                        keys=self.enc,</span><br><span class="line">                                                        num_units=hp.hidden_units,</span><br><span class="line">                                                        num_heads=hp.num_heads,</span><br><span class="line">                                                        dropout_rate=hp.dropout_rate,</span><br><span class="line">                                                        is_training=is_training,</span><br><span class="line">                                                        causality=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">                        <span class="comment">### Feed Forward，接上一个全连接层。</span></span><br><span class="line">                        self.enc = feedforward(self.enc, num_units=[<span class="number">4</span>*hp.hidden_units, hp.hidden_units])</span><br></pre></td></tr></table></figure><h3 id="multihead-attention部分"><a href="#multihead-attention部分" class="headerlink" title="multihead_attention部分"></a>multihead_attention部分</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multihead_attention</span><span class="params">(queries,</span></span></span><br><span class="line"><span class="function"><span class="params">                        keys,</span></span></span><br><span class="line"><span class="function"><span class="params">                        num_units=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                        num_heads=<span class="number">8</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                        dropout_rate=<span class="number">0</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                        is_training=True,</span></span></span><br><span class="line"><span class="function"><span class="params">                        causality=False,</span></span></span><br><span class="line"><span class="function"><span class="params">                        scope=<span class="string">"multihead_attention"</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                        reuse=None)</span>:</span></span><br><span class="line">    <span class="string">'''Applies multihead attention.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args</span></span><br><span class="line"><span class="string">      queries: A 3d tensor with shape of [N, T_q, C_q].</span></span><br><span class="line"><span class="string">      keys: A 3d tensor with shape of [N, T_k, C_k].</span></span><br><span class="line"><span class="string">      num_units: A scalar. Attention size.</span></span><br><span class="line"><span class="string">      dropout_rate: A floating point number.</span></span><br><span class="line"><span class="string">      is_training: Boolean. Controller of mechanism for dropout.</span></span><br><span class="line"><span class="string">      causality: Boolean. If true, units that reference the future are masked.</span></span><br><span class="line"><span class="string">      num_heads: An int. Number of heads.</span></span><br><span class="line"><span class="string">      scope: Optional scope for `variable_scope`.</span></span><br><span class="line"><span class="string">      reuse: Boolean, whether to reuse the weights of a previous layer</span></span><br><span class="line"><span class="string">        by the same name.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">      A 3d tensor with shape of (N, T_q, C)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(scope, reuse=reuse):</span><br><span class="line">        <span class="comment"># Set the fall back option for num_units</span></span><br><span class="line">        <span class="keyword">if</span> num_units <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            num_units = queries.get_shape().as_list[<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Linear projections</span></span><br><span class="line">        Q = tf.layers.dense(queries, num_units, activation=tf.nn.relu) <span class="comment"># (N, T_q, C)</span></span><br><span class="line">        K = tf.layers.dense(keys, num_units, activation=tf.nn.relu) <span class="comment"># (N, T_k, C)</span></span><br><span class="line">        V = tf.layers.dense(keys, num_units, activation=tf.nn.relu) <span class="comment"># (N, T_k, C)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Split and concat</span></span><br><span class="line">        Q_ = tf.concat(tf.split(Q, num_heads, axis=<span class="number">2</span>), axis=<span class="number">0</span>) <span class="comment"># (h*N, T_q, C/h)</span></span><br><span class="line">        K_ = tf.concat(tf.split(K, num_heads, axis=<span class="number">2</span>), axis=<span class="number">0</span>) <span class="comment"># (h*N, T_k, C/h)</span></span><br><span class="line">        V_ = tf.concat(tf.split(V, num_heads, axis=<span class="number">2</span>), axis=<span class="number">0</span>) <span class="comment"># (h*N, T_k, C/h)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Multiplication</span></span><br><span class="line">        outputs = tf.matmul(Q_, tf.transpose(K_, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>])) <span class="comment"># (h*N, T_q, T_k)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Scale</span></span><br><span class="line">        outputs = outputs / (K_.get_shape().as_list()[<span class="number">-1</span>] ** <span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Key Masking</span></span><br><span class="line">        key_masks = tf.sign(tf.abs(tf.reduce_sum(keys, axis=<span class="number">-1</span>))) <span class="comment"># (N, T_k)</span></span><br><span class="line">        key_masks = tf.tile(key_masks, [num_heads, <span class="number">1</span>]) <span class="comment"># (h*N, T_k)</span></span><br><span class="line">        key_masks = tf.tile(tf.expand_dims(key_masks, <span class="number">1</span>), [<span class="number">1</span>, tf.shape(queries)[<span class="number">1</span>], <span class="number">1</span>]) <span class="comment"># (h*N, T_q, T_k)</span></span><br><span class="line"></span><br><span class="line">        paddings = tf.ones_like(outputs)*(<span class="number">-2</span>**<span class="number">32</span>+<span class="number">1</span>)</span><br><span class="line">        outputs = tf.where(tf.equal(key_masks, <span class="number">0</span>), paddings, outputs) <span class="comment"># (h*N, T_q, T_k)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Causality = Future blinding</span></span><br><span class="line">        <span class="keyword">if</span> causality:</span><br><span class="line">            diag_vals = tf.ones_like(outputs[<span class="number">0</span>, :, :]) <span class="comment"># (T_q, T_k)</span></span><br><span class="line">            tril = tf.contrib.linalg.LinearOperatorTriL(diag_vals).to_dense() <span class="comment"># (T_q, T_k)</span></span><br><span class="line">            masks = tf.tile(tf.expand_dims(tril, <span class="number">0</span>), [tf.shape(outputs)[<span class="number">0</span>], <span class="number">1</span>, <span class="number">1</span>]) <span class="comment"># (h*N, T_q, T_k)</span></span><br><span class="line"></span><br><span class="line">            paddings = tf.ones_like(masks)*(<span class="number">-2</span>**<span class="number">32</span>+<span class="number">1</span>)</span><br><span class="line">            outputs = tf.where(tf.equal(masks, <span class="number">0</span>), paddings, outputs) <span class="comment"># (h*N, T_q, T_k)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Activation</span></span><br><span class="line">        outputs = tf.nn.softmax(outputs) <span class="comment"># (h*N, T_q, T_k)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Query Masking</span></span><br><span class="line">        query_masks = tf.sign(tf.abs(tf.reduce_sum(queries, axis=<span class="number">-1</span>))) <span class="comment"># (N, T_q)</span></span><br><span class="line">        query_masks = tf.tile(query_masks, [num_heads, <span class="number">1</span>]) <span class="comment"># (h*N, T_q)</span></span><br><span class="line">        query_masks = tf.tile(tf.expand_dims(query_masks, <span class="number">-1</span>), [<span class="number">1</span>, <span class="number">1</span>, tf.shape(keys)[<span class="number">1</span>]]) <span class="comment"># (h*N, T_q, T_k)</span></span><br><span class="line">        outputs *= query_masks <span class="comment"># broadcasting. (N, T_q, C)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Dropouts</span></span><br><span class="line">        outputs = tf.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(is_training))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Weighted sum</span></span><br><span class="line">        outputs = tf.matmul(outputs, V_) <span class="comment"># ( h*N, T_q, C/h)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Restore shape</span></span><br><span class="line">        outputs = tf.concat(tf.split(outputs, num_heads, axis=<span class="number">0</span>), axis=<span class="number">2</span> ) <span class="comment"># (N, T_q, C)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Residual connection</span></span><br><span class="line">        outputs += queries</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Normalize</span></span><br><span class="line">        outputs = normalize(outputs) <span class="comment"># (N, T_q, C)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure><h3 id="Decoder-部分"><a href="#Decoder-部分" class="headerlink" title="Decoder 部分"></a>Decoder 部分</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Decoder</span></span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">"decoder"</span>):</span><br><span class="line">                <span class="comment">## Embedding</span></span><br><span class="line">                self.dec = embedding(self.decoder_inputs,</span><br><span class="line">                                      vocab_size=len(en2idx),</span><br><span class="line">                                      num_units=hp.hidden_units,</span><br><span class="line">                                      scale=<span class="keyword">True</span>,</span><br><span class="line">                                      scope=<span class="string">"dec_embed"</span>)</span><br><span class="line"></span><br><span class="line">                <span class="comment">## Positional Encoding</span></span><br><span class="line">                self.dec += positional_encoding(self.decoder_inputs,</span><br><span class="line">                                      vocab_size=hp.maxlen,</span><br><span class="line">                                      num_units=hp.hidden_units,</span><br><span class="line">                                      zero_pad=<span class="keyword">False</span>,</span><br><span class="line">                                      scale=<span class="keyword">False</span>,</span><br><span class="line">                                      scope=<span class="string">"dec_pe"</span>)</span><br><span class="line"></span><br><span class="line">                <span class="comment">## Dropout</span></span><br><span class="line">                self.dec = tf.layers.dropout(self.dec,</span><br><span class="line">                                            rate=hp.dropout_rate,</span><br><span class="line">                                            training=tf.convert_to_tensor(is_training))</span><br><span class="line"></span><br><span class="line">                <span class="comment">## Blocks</span></span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> range(hp.num_blocks):</span><br><span class="line">                    <span class="keyword">with</span> tf.variable_scope(<span class="string">"num_blocks_&#123;&#125;"</span>.format(i)):</span><br><span class="line">                        <span class="comment">## Multihead Attention ( self-attention) ，自注意力机制</span></span><br><span class="line">                        self.dec = multihead_attention(queries=self.dec,</span><br><span class="line">                                                        keys=self.dec,</span><br><span class="line">                                                        num_units=hp.hidden_units,</span><br><span class="line">                                                        num_heads=hp.num_heads,</span><br><span class="line">                                                        dropout_rate=hp.dropout_rate,</span><br><span class="line">                                                        is_training=is_training,</span><br><span class="line">                                                        causality=<span class="keyword">True</span>,</span><br><span class="line">                                                        scope=<span class="string">"self_attention"</span>)</span><br><span class="line"></span><br><span class="line">                        <span class="comment">## Multihead Attention ( vanilla attention)，和编码器的输出结果做注意力机制</span></span><br><span class="line">                        self.dec = multihead_attention(queries=self.dec,</span><br><span class="line">                                                        keys=self.enc,</span><br><span class="line">                                                        num_units=hp.hidden_units,</span><br><span class="line">                                                        num_heads=hp.num_heads,</span><br><span class="line">                                                        dropout_rate=hp.dropout_rate,</span><br><span class="line">                                                        is_training=is_training,</span><br><span class="line">                                                        causality=<span class="keyword">False</span>,</span><br><span class="line">                                                        scope=<span class="string">"vanilla_attention"</span>)</span><br><span class="line"></span><br><span class="line">                        <span class="comment">## Feed Forward</span></span><br><span class="line">                        self.dec = feedforward(self.dec, num_units=[<span class="number">4</span>*hp.hidden_units, hp.hidden_units])</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>论文一经提出就收到了巨大的关注，说明Google在该领域的领头羊作用，且2017~2018年出现大量的<code>self-attention</code>文章，说明其有效作用。</p><p>该论文工程性极强，可以看到论文中含有很多的tricks，这些往往影响到最后模型的效果，所以这篇文章还是很值得学习的。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;文章简介&quot;&gt;&lt;a href=&quot;#文章简介&quot; class=&quot;headerlink&quot; title=&quot;文章简介&quot;&gt;&lt;/a&gt;文章简介&lt;/h2&gt;&lt;p&gt;这篇文章是Google公司谷歌大脑部门的一篇关于使用注意力机制&lt;code&gt;（Attention Mechanism）&lt;/code&gt;来进行翻译的模型的研究，这篇文章最终发表在NIPS 2017上。整个模型还是基于 encode-decode的构架，但是attention不再和CNN以及RNN一起使用，而是单独使用。这个框架的名字叫做&lt;code&gt;Transformer&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;论文链接&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="技术" scheme="http://cupdish.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="论文阅读报告" scheme="http://cupdish.com/categories/%E6%8A%80%E6%9C%AF/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E6%8A%A5%E5%91%8A/"/>
    
    
      <category term="深度学习" scheme="http://cupdish.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="论文阅读" scheme="http://cupdish.com/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
      <category term="谷歌" scheme="http://cupdish.com/tags/%E8%B0%B7%E6%AD%8C/"/>
    
      <category term="代码" scheme="http://cupdish.com/tags/%E4%BB%A3%E7%A0%81/"/>
    
      <category term="注意力机制" scheme="http://cupdish.com/tags/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"/>
    
  </entry>
  
  <entry>
    <title>要好好开始写论文了</title>
    <link href="http://cupdish.com/2018/03/24/post/"/>
    <id>http://cupdish.com/2018/03/24/post/</id>
    <published>2018-03-24T04:56:32.000Z</published>
    <updated>2018-11-03T16:17:50.931Z</updated>
    
    <content type="html"><![CDATA[<h2 id="开始好好写论文"><a href="#开始好好写论文" class="headerlink" title="开始好好写论文"></a>开始好好写论文</h2><p>重新配置了一下网站，准备好好地学习写论文和读论文，过一个充实的研究生活</p>]]></content>
    
    <summary type="html">
    
      重新配置了一下网站，准备好好地学习写论文和读论文，过一个充实的研究生活
    
    </summary>
    
      <category term="技术" scheme="http://cupdish.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="数据挖掘" scheme="http://cupdish.com/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
      <category term="论文" scheme="http://cupdish.com/tags/%E8%AE%BA%E6%96%87/"/>
    
      <category term="深度学习" scheme="http://cupdish.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Kobe 投篮预测比赛解题报告</title>
    <link href="http://cupdish.com/2016/06/15/kobe/"/>
    <id>http://cupdish.com/2016/06/15/kobe/</id>
    <published>2016-06-15T00:51:17.000Z</published>
    <updated>2018-11-03T15:10:23.943Z</updated>
    
    <content type="html"><![CDATA[<h2 id="题目数据分析"><a href="#题目数据分析" class="headerlink" title="题目数据分析"></a>题目数据分析</h2><p>这道<a href="https://www.kaggle.com/c/kobe-bryant-shot-selection" target="_blank" rel="noopener">Kaggle题目</a>还是蛮与时俱进的，科比刚刚退役，这道题目就上线了。这道题目主要是把科比在役这么多年以来的比赛投篮数据，主要是NBA的，拿过来，每一项数据就是一次投蓝，当然包括中或者不中，主要是预测科比这次投篮到底是中还是不中。中就是1，不中就是0。每个数据包含了科比的投篮方式，是在什么时间的什么比赛，是不是季后赛，以及投球时所剩余的时间等等信息。从这么多次投篮中，挖去了一些投球结果，比赛的目的就是让参赛选手通过分析没有挖去的训练数据中进行学习和挖掘，然后预测挖去的投球结果到底是中了还是没中。</p><a id="more"></a><p>所有的代码我已经放的我的<a href="https://github.com/ssdf93/kaggle" target="_blank" rel="noopener">Github</a>上面。</p><p>首先，根据<code>shot_made_flag</code>来分数据集和训练集，不为空的数据为训练集，为空的数据为测试集。</p><p>在本文中，我使用的编程语言是<code>Python3.4</code>,使用的包有<code>numpy</code>,<code>pandas</code>,<code>scikit-learn</code>以及<code>Keras</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">if</span> sample:</span><br><span class="line">        data = pd.read_csv(<span class="string">"./data/data_min.csv"</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        data = pd.read_csv(<span class="string">"./data/data.csv"</span>)</span><br><span class="line"></span><br><span class="line">    print(data[<span class="string">'shot_distance'</span>].max())</span><br><span class="line"></span><br><span class="line">    train=data[data[target].notnull()].reset_index(drop=<span class="keyword">True</span>)</span><br><span class="line">    test=data[data[target].isnull()].reset_index(drop=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    print(train.info())</span><br><span class="line">    print(test.info())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> data,train,test</span><br></pre></td></tr></table></figure><p>输出的结果如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">79</span></span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">pandas</span>.<span class="title">core</span>.<span class="title">frame</span>.<span class="title">DataFrame</span>'&gt;</span></span><br><span class="line"><span class="class"><span class="title">RangeIndex</span>:</span> <span class="number">25697</span> entries, <span class="number">0</span> to <span class="number">25696</span></span><br><span class="line">Data columns (total <span class="number">25</span> columns):</span><br><span class="line">action_type           <span class="number">25697</span> non-null object</span><br><span class="line">combined_shot_type    <span class="number">25697</span> non-null object</span><br><span class="line">game_event_id         <span class="number">25697</span> non-null int64</span><br><span class="line">game_id               <span class="number">25697</span> non-null int64</span><br><span class="line">lat                   <span class="number">25697</span> non-null float64</span><br><span class="line">loc_x                 <span class="number">25697</span> non-null int64</span><br><span class="line">loc_y                 <span class="number">25697</span> non-null int64</span><br><span class="line">lon                   <span class="number">25697</span> non-null float64</span><br><span class="line">minutes_remaining     <span class="number">25697</span> non-null int64</span><br><span class="line">period                <span class="number">25697</span> non-null int64</span><br><span class="line">playoffs              <span class="number">25697</span> non-null int64</span><br><span class="line">season                <span class="number">25697</span> non-null object</span><br><span class="line">seconds_remaining     <span class="number">25697</span> non-null int64</span><br><span class="line">shot_distance         <span class="number">25697</span> non-null int64</span><br><span class="line">shot_made_flag        <span class="number">25697</span> non-null float64</span><br><span class="line">shot_type             <span class="number">25697</span> non-null object</span><br><span class="line">shot_zone_area        <span class="number">25697</span> non-null object</span><br><span class="line">shot_zone_basic       <span class="number">25697</span> non-null object</span><br><span class="line">shot_zone_range       <span class="number">25697</span> non-null object</span><br><span class="line">team_id               <span class="number">25697</span> non-null int64</span><br><span class="line">team_name             <span class="number">25697</span> non-null object</span><br><span class="line">game_date             <span class="number">25697</span> non-null object</span><br><span class="line">matchup               <span class="number">25697</span> non-null object</span><br><span class="line">opponent              <span class="number">25697</span> non-null object</span><br><span class="line">shot_id               <span class="number">25697</span> non-null int64</span><br><span class="line">dtypes: float64(<span class="number">3</span>), int64(<span class="number">11</span>), object(<span class="number">11</span>)</span><br><span class="line">memory usage: <span class="number">4.9</span>+ MB</span><br><span class="line"><span class="keyword">None</span></span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">pandas</span>.<span class="title">core</span>.<span class="title">frame</span>.<span class="title">DataFrame</span>'&gt;</span></span><br><span class="line"><span class="class"><span class="title">RangeIndex</span>:</span> <span class="number">5000</span> entries, <span class="number">0</span> to <span class="number">4999</span></span><br><span class="line">Data columns (total <span class="number">25</span> columns):</span><br><span class="line">action_type           <span class="number">5000</span> non-null object</span><br><span class="line">combined_shot_type    <span class="number">5000</span> non-null object</span><br><span class="line">game_event_id         <span class="number">5000</span> non-null int64</span><br><span class="line">game_id               <span class="number">5000</span> non-null int64</span><br><span class="line">lat                   <span class="number">5000</span> non-null float64</span><br><span class="line">loc_x                 <span class="number">5000</span> non-null int64</span><br><span class="line">loc_y                 <span class="number">5000</span> non-null int64</span><br><span class="line">lon                   <span class="number">5000</span> non-null float64</span><br><span class="line">minutes_remaining     <span class="number">5000</span> non-null int64</span><br><span class="line">period                <span class="number">5000</span> non-null int64</span><br><span class="line">playoffs              <span class="number">5000</span> non-null int64</span><br><span class="line">season                <span class="number">5000</span> non-null object</span><br><span class="line">seconds_remaining     <span class="number">5000</span> non-null int64</span><br><span class="line">shot_distance         <span class="number">5000</span> non-null int64</span><br><span class="line">shot_made_flag        <span class="number">0</span> non-null float64</span><br><span class="line">shot_type             <span class="number">5000</span> non-null object</span><br><span class="line">shot_zone_area        <span class="number">5000</span> non-null object</span><br><span class="line">shot_zone_basic       <span class="number">5000</span> non-null object</span><br><span class="line">shot_zone_range       <span class="number">5000</span> non-null object</span><br><span class="line">team_id               <span class="number">5000</span> non-null int64</span><br><span class="line">team_name             <span class="number">5000</span> non-null object</span><br><span class="line">game_date             <span class="number">5000</span> non-null object</span><br><span class="line">matchup               <span class="number">5000</span> non-null object</span><br><span class="line">opponent              <span class="number">5000</span> non-null object</span><br><span class="line">shot_id               <span class="number">5000</span> non-null int64</span><br><span class="line">dtypes: float64(<span class="number">3</span>), int64(<span class="number">11</span>), object(<span class="number">11</span>)</span><br><span class="line">memory usage: <span class="number">976.6</span>+ KB</span><br><span class="line"><span class="keyword">None</span></span><br></pre></td></tr></table></figure><p>从上表可以看出，数据比较完整，不需要做过多的清洗和整理，因此，我没有去做过多的数据预处理。</p><p>最后的数据提交的是判断科比投篮投中的概率，0~1之间的一个数值，使用了<code>logLoss</code>的进行评估。<em>所以，提交的时候尽量交概率值，而不是分类值</em></p><h2 id="特征构造"><a href="#特征构造" class="headerlink" title="特征构造"></a>特征构造</h2><p>接下来就是数据的特征抽取和构造，这里，主要是对数据的提取和凝练，有些数据还是需要进行一些处理，比如将类别数据转换成哑变量，将数据归一化等等。</p><p>首先是对特征的构造，这里主要构造了时间相关的数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> [train,test]:</span><br><span class="line">        data[<span class="string">'month'</span>]=data[<span class="string">'game_date'</span>].apply(<span class="keyword">lambda</span> x: get_date(x,<span class="string">'month'</span>))</span><br><span class="line">        data[<span class="string">'year'</span>]=data[<span class="string">'game_date'</span>].apply(<span class="keyword">lambda</span> x: get_date(x,<span class="string">'year'</span>))</span><br><span class="line">        data[<span class="string">'day'</span>]=data[<span class="string">'game_date'</span>].apply(<span class="keyword">lambda</span> x: get_date(x,<span class="string">'day'</span>))</span><br><span class="line">        data[<span class="string">'range'</span>]=data[<span class="string">'shot_zone_range'</span>].map(&#123;<span class="string">'16-24 ft.'</span>:<span class="number">20</span>, <span class="string">'8-16 ft.'</span>:<span class="number">12</span>, <span class="string">'Less Than 8 ft.'</span>:<span class="number">4</span>, <span class="string">'24+ ft.'</span>:<span class="number">28</span>, <span class="string">'Back Court Shot'</span>:<span class="number">36</span>&#125;)</span><br><span class="line">        data[<span class="string">'season'</span>]=data[<span class="string">'season'</span>].apply(<span class="keyword">lambda</span> x: int(x.split(<span class="string">'-'</span>)[<span class="number">1</span>]))</span><br><span class="line">        data[<span class="string">'is_host'</span>]=data[<span class="string">'matchup'</span>].apply(<span class="keyword">lambda</span> x: <span class="number">1</span> <span class="keyword">if</span> <span class="string">'@'</span> <span class="keyword">in</span> x <span class="keyword">else</span> <span class="number">0</span>)</span><br><span class="line">        data[<span class="string">'shot_type'</span>]=data[<span class="string">'shot_type'</span>].apply(<span class="keyword">lambda</span> x: <span class="number">1</span> <span class="keyword">if</span> <span class="string">'2'</span> <span class="keyword">in</span> x <span class="keyword">else</span> <span class="number">0</span>)</span><br><span class="line">        data[<span class="string">'is_addTime'</span>]=data[<span class="string">'period'</span>].apply(<span class="keyword">lambda</span> x:<span class="number">1</span> <span class="keyword">if</span> x&gt;<span class="number">4</span> <span class="keyword">else</span> <span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>上述代码主要对时间数据进行了解析，然后提取了赛季数据，主客场，是否是2分投球，是否是加时赛等等。</p><p>另外，对多个类别的数据进行哑变量处理。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">dummy_variables=[<span class="string">'action_type'</span>,<span class="string">'combined_shot_type'</span>,<span class="string">'shot_zone_area'</span>,<span class="string">'shot_zone_basic'</span>,<span class="string">'opponent'</span>]</span><br><span class="line">    <span class="keyword">for</span> var <span class="keyword">in</span> dummy_variables:</span><br><span class="line">        encoded = pd.get_dummies(pd.concat([train,test], axis=<span class="number">0</span>)[var],prefix=var)</span><br><span class="line">        train_rows = train.shape[<span class="number">0</span>]</span><br><span class="line">        train_encoded = encoded.iloc[:train_rows, :]</span><br><span class="line">        test_encoded = encoded.iloc[train_rows:, :]</span><br><span class="line">        features+=encoded.columns.tolist()</span><br><span class="line">        train = pd.concat([train, train_encoded], axis=<span class="number">1</span>)</span><br><span class="line">        test = pd.concat([test,test_encoded],axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>也有一种方法是对这些类别的数据经行一个有序的编号，比如从0~n，但是这样的编号会给后面的分类过程引入多余的信息，也就是给他们一个顺序的信息，而大多数情况下，这些类别数据是没有顺序关系的。我之前使用的是编号方法，结果是<code>0.607</code>，而使用哑变量的结果大大提高，结果到达了<code>0.60061</code>。看来对于这种类别型的数据，使用哑变量还是有效果的。</p><p>然后加了一些原始的特征，如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">features+=[<span class="string">'loc_x'</span>,<span class="string">'loc_y'</span>,<span class="string">'period'</span>,<span class="string">'minutes_remaining'</span>,<span class="string">'seconds_remaining'</span>,<span class="string">'shot_distance'</span>,<span class="string">'month'</span>,<span class="string">'year'</span>,<span class="string">'day'</span>,<span class="string">'season'</span>,<span class="string">'range'</span>,<span class="string">'is_host'</span>,<span class="string">'shot_type'</span>,<span class="string">'is_addTime'</span>]</span><br></pre></td></tr></table></figure><p>接下来是数据的标准化或者归一化，我使用<code>sklearn</code>中的<code>sklearn.preprocessing.StandardScaler</code>和<code>sklearn.preprocessing.MinMaxScaler</code>两个不同的函数，发现这连个函数对结果的影响不大，最终选择了归一化到0~1之间。</p><h2 id="使用模型"><a href="#使用模型" class="headerlink" title="使用模型"></a>使用模型</h2><p>这是一道明显的二分类问题，一般使用分类算法来进行计算，我选择使用的算法有<code>xgboost</code>，<code>neural networks</code>等等</p><h3 id="xgboost"><a href="#xgboost" class="headerlink" title="xgboost"></a>xgboost</h3><p>现在我感觉基本上<code>xgboost</code>都是数据竞赛的首选分类算法。这是<a href="https://xgboost.readthedocs.io/en/latest/" target="_blank" rel="noopener">文档</a>。可以使用<code>xgb.cv</code>来进行交叉验证和调参，也可以添加<code>watchlist</code>来进行调参。调参的方法可以人工调，也可以通过使用<strong>网格搜索</strong>或者<strong>随机搜索</strong>来进行参数搜索。</p><p>我是用了以下的参数进行了计算，如果需要复现，可能需要根据具体情况进行调参。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">params = &#123;<span class="string">'max_depth'</span>:<span class="number">8</span>, <span class="string">'eta'</span>:<span class="number">0.02</span>,<span class="string">'silent'</span>:<span class="number">1</span>,</span><br><span class="line">          <span class="string">'objective'</span>:<span class="string">'binary:logistic'</span>, <span class="string">'eval_metric'</span>: <span class="string">'logloss'</span>,</span><br><span class="line">          <span class="string">'min_child_weight'</span>:<span class="number">3</span>, <span class="string">'subsample'</span>:<span class="number">0.5</span>,<span class="string">'colsample_bytree'</span>:<span class="number">0.5</span>, <span class="string">'nthread'</span>:<span class="number">4</span>&#125;</span><br><span class="line">num_rounds = <span class="number">290</span></span><br></pre></td></tr></table></figure><p>然后开始训练模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">watchlist = [(xgbdev,<span class="string">'eval'</span>), (xgbtrain,<span class="string">'train'</span>)]</span><br><span class="line">evals_result = &#123;&#125;</span><br><span class="line">classifier = xgb.train(params, xgbtrain, num_rounds, watchlist)</span><br></pre></td></tr></table></figure><p>最后进行测试和输出<br>当然，我也是用了其他的模型方法</p><h3 id="neural-networks"><a href="#neural-networks" class="headerlink" title="neural networks"></a>neural networks</h3><p>我主要使用了<a href="http://keras.io/" target="_blank" rel="noopener">Keras</a>来实现神经网络，<code>Keras</code>是一个python的神经网络库，它的backend可以选择为<code>Theano</code>或<code>TensorFlow</code>，最近出的<code>functional api</code>感觉非常不错。以下是我的keras的代码，我尝试着使用了1~2层隐藏层，隐藏层的节点数选择了<code>50</code>，<code>100</code>等参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">features_cnt = len(features)</span><br><span class="line">inputs = Input(shape=(features_cnt,))</span><br><span class="line">dense1 = Dense(<span class="number">100</span>, activation=<span class="string">'relu'</span>)(inputs)</span><br><span class="line">dropout1 = Dropout(<span class="number">0.5</span>)(dense1)</span><br><span class="line">outputs = Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>)(dropout1)</span><br><span class="line">model = Model(input=inputs, output=outputs)</span><br><span class="line">model.compile(loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">              optimizer=<span class="string">'sgd'</span>, metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line">print(<span class="string">"Start Training"</span>, time.ctime())</span><br><span class="line">model.fit(train[features].values, train[target].values,</span><br><span class="line">          batch_size=<span class="number">12</span>, nb_epoch=<span class="number">100</span>, validation_split=<span class="number">0.25</span>)</span><br></pre></td></tr></table></figure><h2 id="组合模型"><a href="#组合模型" class="headerlink" title="组合模型"></a>组合模型</h2><p>在这里，我只是用了<code>bagging</code>的方法，我用之前的模型获得了5个结果。结果列表如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">files=[<span class="string">'xgboost-feature-submit_best.csv'</span>,<span class="string">'xgboost-dummy-feature-best.csv'</span>,<span class="string">'nn-feature-submit.csv'</span>,<span class="string">'nn-tanh-feature-submit.csv'</span>,<span class="string">'xgboost-linear-feature-submit.csv'</span>]</span><br></pre></td></tr></table></figure><p>这五个模型分别是xgboost调参后最好的模型（没有将类别变量变为哑变量），变为哑变量后的最好结果，神经网络的结果（使用relu激活函数），使用tanh激活函数的结果，使用线性方法的xgboost结果</p><p>其权重以及<code>Public Leaderboard</code>结果：</p><ul><li><code>weight=[0.15,0.6,0.1,0.15,0.1]</code> 结果 <code>0.62815</code></li><li><code>weight=[0.05,0.7,0.1,0.05,0.1]</code> 结果 <code>0.60009</code></li><li><code>weight=[0.05,0.75,0.1,0.05,0.05]</code> 结果 <code>0.59987</code></li><li><code>weight=[0.05,0.8,0.05,0.05,0.05]</code> 结果 <code>0.60008</code></li><li><code>weight=[0.04,0.73,0.15,0.04,0.04]</code> 结果 <code>0.59970</code></li><li><code>weight=[0.04,0.7,0.18,0.04,0.04]</code> 结果 <code>0.59962</code></li><li><code>weight=[0.04,0.6,0.28,0.04,0.04]</code> 结果 <code>0.59952</code></li><li><code>weight=[0.05,0.55,0.30,0.05,0]</code> 结果 <code>0.59959</code></li><li><code>weight=[0.01,0.55,0.36,0.04,0.04]</code> 结果 <code>0.59953</code></li><li><code>weight=[0.01,0.55,0.38,0.03,0.03]</code> 结果 <code>0.59951</code></li></ul><p>结论，<code>bagging</code>的方法还是有一定作用的，毕竟单个模型还没有低于<code>0.6</code>的</p><h2 id="解题心得"><a href="#解题心得" class="headerlink" title="解题心得"></a>解题心得</h2><h3 id="学到的知识"><a href="#学到的知识" class="headerlink" title="学到的知识"></a>学到的知识</h3><ul><li>对于<code>pandas</code>的操作更熟练了</li><li>对于<code>哑变量</code>有了更深刻的理解</li><li>对于<code>bagging</code>的方法有了更深入的了解，<code>bagging</code>的确可以提高准确率</li></ul><h3 id="不足"><a href="#不足" class="headerlink" title="不足"></a>不足</h3><ul><li>前期对于数据的分析还不够，感觉就是搬个模型在套，如果能够分析好数据，解题的过程会更加地有方向</li><li>没有考虑其他<code>集成学习</code>的算法</li><li>没有考虑其他的算法，如<code>KNN</code>，<code>SVM</code>等</li></ul>]]></content>
    
    <summary type="html">
    
      这篇文章主要介绍kaggle上的Kobe Bryant Shot Selection 这道题，是预测科比每次投篮的命中情况预测
    
    </summary>
    
      <category term="数据挖掘" scheme="http://cupdish.com/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
    
      <category term="kaggle" scheme="http://cupdish.com/tags/kaggle/"/>
    
      <category term="Kobe" scheme="http://cupdish.com/tags/Kobe/"/>
    
      <category term="结题报告" scheme="http://cupdish.com/tags/%E7%BB%93%E9%A2%98%E6%8A%A5%E5%91%8A/"/>
    
  </entry>
  
  <entry>
    <title>KDDCup2016 总结</title>
    <link href="http://cupdish.com/2016/05/29/KDDCup2016-%E6%80%BB%E7%BB%93/"/>
    <id>http://cupdish.com/2016/05/29/KDDCup2016-总结/</id>
    <published>2016-05-29T07:52:41.000Z</published>
    <updated>2018-03-25T01:07:39.003Z</updated>
    
    <content type="html"><![CDATA[<h2 id="KDDCUP-总览与概况"><a href="#KDDCUP-总览与概况" class="headerlink" title="KDDCUP 总览与概况"></a>KDDCUP 总览与概况</h2><p>今天已经提交了最后一次kddcup phrase3 的结果。目前为止，今年的kddcup也算是结束，这也是我第一次参加kddcup。在这次kddcup结果出来之前，也只有静静地等待。对于这次的kddcup有一些感想，记录下来。</p><a id="more"></a><p>这次的kddcup2016和以往的有所不同，也和其他kaggle竞赛也有不同。这次的比赛纯粹的是一道预测题，连主办方也没有正确答案。其他类型的题目多是主办方已经有正确答案，整个过程就是离线测试，看看谁的结果好。</p><p>这次的比赛是预测2016年若干个国际顶级会议上，全世界各个机构的排名。排名的结果根据该机构下发表文章的作者数与论文数据有关。假设某会议录取的论文中，每篇论文的得分为1，该得分平均分给每个作者，每个作者再将其得分平均分给其所在机构。</p><p>需要预测的8歌会议分别是<br><strong>SIGIR</strong>: International ACM SIGIR Conference on Research and Development in Information Retrieval<br><strong>SIGMOD</strong>: ACM SIGMOD International Conference on Management of Data<br><strong>SIGCOMM</strong>: ACM SIGCOMM Annual Conference on Data Communication<br><strong>KDD</strong>: ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<br><strong>ICML</strong>: International Conference on Machine Learning<br><strong>FSE</strong>: ACM SIGSOFT International Symposium on the Foundations of Software Engineering<br><strong>MobiCom</strong>: The Annual International Conference on Mobile Computing and Networking<br><strong>MM</strong>: ACM international conference on Multimedia</p><p>目前数据给的是2011-2015计算机会议（包含上述8个会议）的论文，会议，作者，机构等信息，（不包含论文具体的内容及摘要，也不包含机构的地点信息）。也就是对于2016年的信息几乎寥寥无几，不过组委会鼓励大家搜集公开的其他数据。</p><p>这道题根据各个阶段时间截止后公布的会议中的一个会议的结果对该阶段进行排序。目前已知第一阶段是SIGIR，第二阶段是KDD。而在比赛阶段使用leaderboard进行实时评估，评估的依据是根据所有人提交的结果计算出平均结果，然后把这个平均结果跟你的结果进行比较，只比较排名。 <strong>但是</strong>，这个结果只供参考，最后的结果以真实的Ground Truth结果为准。</p><p>而leaderboard的结果和真实的结果差距大吗？根据第一阶段第二阶段的结果显示，两者差距相当大！第一阶段SIGIR的前几名都是在leaderboard位于一百名之后。因此，这次比赛的结果表明，<strong>即使你再排名中位于3%内，也不见得会在最终的排名中位于20%内</strong>，也就是leaderboard结果和真实的结果有巨大的偏差。不过好在<strong>在最终结果中排前几名的队伍，在leaderboard也保持很高的得分</strong>。而根据得分的计算规则，只考虑前20个机构，而且位于前几名的机构占更大的比重，通常，一些有名的机构会经常占据每年会议前几名。</p><h2 id="kddcup的数据简要分析"><a href="#kddcup的数据简要分析" class="headerlink" title="kddcup的数据简要分析"></a>kddcup的数据简要分析</h2><p>这次数据量还是比较大的，所有数据的压缩包是23G左右，而解压之后到60~80G。好在数据的格式非常好，易于操作，开始的时候我是用的是Spark处理的数据，后来发现Python已经能够做简单的筛选工作，而且速度还不错。</p><p>最容易想到的是，只提取和那8个顶级会议的数据进行操作，根据2016KDDCupSelectedAffiliations， 2016KDDCupSelectedPapers提到的机构和文章对其他的数据进行筛选，把60~80G的数据提取到了2-3M，当然这样也丢失了大量的信息。</p><p>根据题目的要去算出各个机构在某年某会议上的得分。</p><h2 id="kddcup第一阶段的分析"><a href="#kddcup第一阶段的分析" class="headerlink" title="kddcup第一阶段的分析"></a>kddcup第一阶段的分析</h2><p>参加第一阶段的时候，我们的时间就不多了，那个时候也不是全力在搞kdd这个事，主要是前期在纠结数据处理上花了很多时间，包括布局spark和hdfs，其实后来发现不用spark单机处理这些数据完全没问题。</p><p>根据我们算出来的机构得分，我们很快使用 2011-2015年的得分数据做已知量，线性拟合出线性的表达式，求出2016年得分。提交结果前位于100名左右，而提交结果之后位于130名左右。</p><h2 id="kddcup第二阶段分析"><a href="#kddcup第二阶段分析" class="headerlink" title="kddcup第二阶段分析"></a>kddcup第二阶段分析</h2><p>第二阶段开始后一直没动，主要是在等第一阶段的groundtruth。第一阶段的groudtruth出来后，结果不好，第二阶段决定换换套路，首先使用的是机器学习的方法<br>准备拿11-14年的数据，预测15年的数据。拿12-15年的数据来预测16年的数据，提取了各年排名，得分数据，使用了SVR的回归，效果不好，于是觉得抛弃。</p><p>后来想到，对以历史数据来说，在2015，2014年表现的好的，可能今年表现的会更好，2011，2012，2013也会对今年有影响，但是可能影响不及去年和前年，于是按照权重由小到大对于2011-2015的分数加权求和，作为2016年分数，然后归一化作为结果提交上去。根据leaderboard稍微调节一下参数。</p><p>结果是在leaderboard排名60左右，在最终的groundTruth达到前top5以内。</p><h2 id="第三阶段分析"><a href="#第三阶段分析" class="headerlink" title="第三阶段分析"></a>第三阶段分析</h2><p>第三阶段算是做的比较认真的一个阶段了，尝试了各种不同的方法</p><h3 id="方法一：线性回归"><a href="#方法一：线性回归" class="headerlink" title="方法一：线性回归"></a>方法一：线性回归</h3><p>提取不同的参数，不同的特征，进行线性回归，但是效果不好，$R^2$的值太小，可解释性不好，最终的结果也不好。</p><h3 id="方法二：线性加权"><a href="#方法二：线性加权" class="headerlink" title="方法二：线性加权"></a>方法二：线性加权</h3><p>线性加权在leaderboard的效果一直不错，我一直在怀疑这种简单的模型是不是可以产生比较好的结果，毕竟大的趋势上线性加权的效果不错。</p><h3 id="方法三：分类"><a href="#方法三：分类" class="headerlink" title="方法三：分类"></a>方法三：分类</h3><p>这次我把数据分的更细了，2011-2013的数据作为已知数据，2014年的得分对应为target，2012-2014为已知数据，2015年为target，这两个作为训练集。而2013-2015年作为测试集，2016年作为target。训练集分为训练数据和交叉验证数据。一般是5fold。</p><p>特征提取了，这些年的依次发表的文章数，得分，排名，作者数，发布多篇文章的作者数，引文数量，被英文数量，以及会议热词（整个会议中的关键词出现的次数较高的词）出现的次数等等。</p><p>去进前30名（或者其他）为1，没进的为0，而通常，这个数据的true positive的数据往往比较低，也就是说分类器分的不好，即使正确率很高。</p><p>因此，分类没继续做下去了</p><h3 id="方法四：回归"><a href="#方法四：回归" class="headerlink" title="方法四：回归"></a>方法四：回归</h3><p>回归也是使用分类的思路，使用了三个回归算法，SVR，GradientBoostingRegressor和LinearRegression，发现使用GBR的效果稍好。</p><h3 id="方法五：集成学习"><a href="#方法五：集成学习" class="headerlink" title="方法五：集成学习"></a>方法五：集成学习</h3><p>把上一个方法的三个结果合并在一起，发现效果稍微有所提高。具体的是使用voting的方法，把上面单个回归所得到的结果从小到大排序，取前50名，第一名得分1.00，第二名得分0.98分，依次类推，然后将得分相加求平均，最后的结果作为集成学习提交，的确比单个回归的效果要好。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>最终第三阶段还是使用了线性加权。<br>不足：<br>[1] 只考虑了很小一部分数据，而对于大部分数据都直接扔掉了，数据利用率较低。<br>[2] 对数据特征的挖掘还不够透彻。<br>[3] 虽然数据量较多，但是一旦以机构来作为样例，数据量还是偏少，应该从其他角度考虑。<br>[4] 只使用了简单的算法，没有考虑更复杂的算法。<br>[5] 对数据的操作还有些欠缺。<br>[6] 投入到竞赛的时间还不够。</p>]]></content>
    
    <summary type="html">
    
      这篇文章主要总结了本人组队参加KDDCup2016的部分收货总结与感悟
    
    </summary>
    
      <category term="数据挖掘" scheme="http://cupdish.com/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
    
      <category term="数据挖掘" scheme="http://cupdish.com/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
      <category term="kaggle" scheme="http://cupdish.com/tags/kaggle/"/>
    
      <category term="竞赛" scheme="http://cupdish.com/tags/%E7%AB%9E%E8%B5%9B/"/>
    
  </entry>
  
</feed>
